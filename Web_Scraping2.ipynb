{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h1>An Interesting Title</h1>\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "html = urlopen('http://www.pythonscraping.com/pages/warandpeace.html')\n",
    "bsObj = BeautifulSoup(html.read(), 'html5lib')\n",
    "print(bsObj.h1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any of the following can produce the same output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<h1>An Interesting Title</h1>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bsObj.html.body.h1\n",
    "bsObj.body.h1\n",
    "bsObj.html.h1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bsObj.tagName get the **first occurrence** of the tag found on the page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Handling\n",
    "* The page is not found on the server\n",
    "  - the error may be '404 Page Not Found'\n",
    "  - urlopen throw 'HTTPError'\n",
    "* The server is not found\n",
    "  - urlopen throw 'URLError'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The server could not be found!\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from urllib.error import HTTPError\n",
    "from urllib.error import URLError\n",
    "\n",
    "try:\n",
    "    html = urlopen('http://www.danielmao.com/')\n",
    "except HTTPError as e:\n",
    "    print(e)\n",
    "except URLError as e:\n",
    "    print('The server could not be found!')\n",
    "else:\n",
    "    print('It worked!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides, even the page is retrived successfully, it may still return error if the tag doesn't exists.  \n",
    "If you call a tag that does not exist, the **Noneobject** is returned. It returns an attributeError if you retrieve another tag from the **Noneobject**.  \n",
    "**print(bsObj.nonExistentTag.someTag)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tag was not found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\daniemao\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\bs4\\element.py:1050: UserWarning: .nonExistingTag is deprecated, use .find(\"nonExisting\") instead.\n",
      "  tag_name, tag_name))\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    badContent = bsObj.nonExistingTag.anotherTag\n",
    "except AttributeError as e:\n",
    "    print('Tag was not found')\n",
    "else: \n",
    "    if badContent == None:\n",
    "        print('Tag was not found')\n",
    "    else:\n",
    "        print(badContent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced HTML Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "find() and findAll() are two functions that easily filter HTML pages using tags and their various attributes.  \n",
    "  \n",
    "**findAll(tag, attributes, recursive, text, limit, keywords)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an **bad** practice. Tiny change on the web can break the program. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bsObj.findAll('table')[4].findAll('tr')[2].find('td').findAll('div')[1].find['a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anna\n",
      "Pavlovna Scherer\n",
      "Empress Marya\n",
      "Fedorovna\n",
      "Prince Vasili Kuragin\n",
      "Anna Pavlovna\n",
      "St. Petersburg\n",
      "the prince\n",
      "Anna Pavlovna\n",
      "Anna Pavlovna\n",
      "the prince\n",
      "the prince\n",
      "the prince\n",
      "Prince Vasili\n",
      "Anna Pavlovna\n",
      "Anna Pavlovna\n",
      "the prince\n",
      "Wintzingerode\n",
      "King of Prussia\n",
      "le Vicomte de Mortemart\n",
      "Montmorencys\n",
      "Rohans\n",
      "Abbe Morio\n",
      "the Emperor\n",
      "the prince\n",
      "Prince Vasili\n",
      "Dowager Empress Marya Fedorovna\n",
      "the baron\n",
      "Anna Pavlovna\n",
      "the Empress\n",
      "the Empress\n",
      "Anna Pavlovna's\n",
      "Her Majesty\n",
      "Baron\n",
      "Funke\n",
      "The prince\n",
      "Anna\n",
      "Pavlovna\n",
      "the Empress\n",
      "The prince\n",
      "Anatole\n",
      "the prince\n",
      "The prince\n",
      "Anna\n",
      "Pavlovna\n",
      "Anna Pavlovna\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "html = urlopen('http://www.pythonscraping.com/pages/warandpeace.html')\n",
    "bsObj = BeautifulSoup(html, \"html5lib\")\n",
    "\n",
    "nameList = bsObj.findAll('span', {'class': 'green'})\n",
    "for name in nameList:\n",
    "    print(name.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrap the product title and price for website http://www.pythonscraping.com/pages/page3.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "html = urlopen('http://www.pythonscraping.com/pages/page3.html')\n",
    "bsObj = BeautifulSoup(html.read(), 'html5lib')\n",
    "data = []\n",
    "\n",
    "for tr in bsObj.findAll('tr', {'class': 'gift'}):\n",
    "    data.append((tr.findAll('td')[0].get_text().strip(), tr.findAll('td')[2].get_text().strip()))\n",
    "\n",
    "with open('test.csv', 'a') as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    for title, price in data:\n",
    "        writer.writerow([title, price])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regular expressions and BeautifulSoup go hand in hand when it comes to scraping the Web.  \n",
    "Most functions that take in a string argument (e.g., find(id = 'aTagIdHere')) will also take in a regular expression as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../img/gifts/img1.jpg\n",
      "../img/gifts/img2.jpg\n",
      "../img/gifts/img3.jpg\n",
      "../img/gifts/img4.jpg\n",
      "../img/gifts/img6.jpg\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "html = urlopen('http://www.pythonscraping.com/pages/page3.html')\n",
    "bsObj = BeautifulSoup(html.read(), 'html5lib')\n",
    "images = bsObj.findAll('img', {'src': re.compile('\\.\\.\\/img\\/gifts/img.*\\.jpg')})\n",
    "for image in images:\n",
    "    print(image['src'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even below code generate the same results as the above code, the attribute defined by regualar expression narrow down the filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../img/gifts/logo.jpg\n",
      "../img/gifts/img1.jpg\n",
      "../img/gifts/img2.jpg\n",
      "../img/gifts/img3.jpg\n",
      "../img/gifts/img4.jpg\n",
      "../img/gifts/img6.jpg\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "html = urlopen('http://www.pythonscraping.com/pages/page3.html')\n",
    "bsObj = BeautifulSoup(html.read(), 'html5lib')\n",
    "images = bsObj.findAll('img')\n",
    "for image in images:\n",
    "    print(image['src'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
